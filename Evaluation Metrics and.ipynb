{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a66e040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable\\n that is predictable from the independent variable(s). It represents the goodness of fit of the model.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. What does R-squared represent in a regression model \n",
    "'''R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable\n",
    " that is predictable from the independent variable(s). It represents the goodness of fit of the model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dbbb244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The assumptions of linear regression include:\\n\\nLinearity: The relationship between the independent variable(s) and the dependent variable should be linear.\\nIndependence: Each observation should be independent of the others.\\nHomoscedasticity: The variance of the residuals should be constant across all levels of the independent variable(s).\\nNormality: The residuals should be normally distributed.\\nNo multicollinearity: The independent variables should not be highly correlated with each other.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. What are the assumptions of linear regression: \n",
    "'''The assumptions of linear regression include:\n",
    "\n",
    "Linearity: The relationship between the independent variable(s) and the dependent variable should be linear.\n",
    "Independence: Each observation should be independent of the others.\n",
    "Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variable(s).\n",
    "Normality: The residuals should be normally distributed.\n",
    "No multicollinearity: The independent variables should not be highly correlated with each other.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7093357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" R-squared measures the proportion of variance explained by the model, while Adjusted R-squared adjusts for the number of independent variables in the model,\\n providing a more accurate estimate of the model's performance.\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. What is the difference between R-squared and Adjusted R-squared:\n",
    "''' R-squared measures the proportion of variance explained by the model, while Adjusted R-squared adjusts for the number of independent variables in the model,\n",
    " providing a more accurate estimate of the model's performance.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9176fb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" MSE measures the average squared difference between predicted and actual values, providing a measure of the model's accuracy and \\na way to compare the performance of different models.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Why do we use Mean Squared Error (MSE)\n",
    "\n",
    "''' MSE measures the average squared difference between predicted and actual values, providing a measure of the model's accuracy and \n",
    "a way to compare the performance of different models.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01593802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' An Adjusted R-squared value of 0.85 indicates that approximately 85% of the variance in the dependent variable is explained by the independent variable(s),\\n after adjusting for the number of independent variables in the model.\\n '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. What does an Adjusted R-squared value of 0.85 indicate\n",
    "''' An Adjusted R-squared value of 0.85 indicates that approximately 85% of the variance in the dependent variable is explained by the independent variable(s),\n",
    " after adjusting for the number of independent variables in the model.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b51dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nWe can check for normality of residuals using:\\n\\nQ-Q plots\\nHistograms of residuals\\nShapiro-Wilk test\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. How do we check for normality of residuals in linear regression: \n",
    "''''\n",
    "We can check for normality of residuals using:\n",
    "\n",
    "Q-Q plots\n",
    "Histograms of residuals\n",
    "Shapiro-Wilk test'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e70929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multicollinearity occurs when two or more independent variables are highly correlated with each other. \\nThis can lead to unstable estimates of the regression coefficients and inflated standard errors.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. What is multicollinearity, and how does it impact regression \n",
    "\n",
    "'''Multicollinearity occurs when two or more independent variables are highly correlated with each other. \n",
    "This can lead to unstable estimates of the regression coefficients and inflated standard errors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1664fd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMAE measures the average absolute difference between predicted and actual values, providing a measure of the model's accuracy.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8. What is Mean Absolute Error (MAE)\n",
    "\n",
    "'''\n",
    "MAE measures the average absolute difference between predicted and actual values, providing a measure of the model's accuracy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9a5477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The benefits of using an ML pipeline include:\\n\\nImproved reproducibility\\nIncreased efficiency\\nBetter organization and management of data and code'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9. What are the benefits of using an ML pipeline\n",
    "\n",
    "'''  The benefits of using an ML pipeline include:\n",
    "\n",
    "Improved reproducibility\n",
    "Increased efficiency\n",
    "Better organization and management of data and code'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa0f569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RMSE is considered more interpretable than MSE because it has the same units as the dependent variable, making it easier to understand and compare the model's performance.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10. Why is RMSE considered more interpretable than MSE\n",
    "\n",
    "'''RMSE is considered more interpretable than MSE because it has the same units as the dependent variable, making it easier to understand and compare the model's performance.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600b2924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Pickling is a way to serialize and save Python objects, including ML models, to a file. This allows us to save and load ML models for later use.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#11. What is pickling in Python, and how is it useful in ML\n",
    "\n",
    "'''  Pickling is a way to serialize and save Python objects, including ML models, to a file. This allows us to save and load ML models for later use.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "400732c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A high R-squared value (close to 1) indicates that the model explains a large proportion of the variance in the dependent variable.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#12. What does a high R-squared value mean\n",
    "''' A high R-squared value (close to 1) indicates that the model explains a large proportion of the variance in the dependent variable.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688156d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" If linear regression assumptions are violated, the model's estimates and predictions may be biased or unreliable. \\nWe need to check and address any violations before interpreting the results.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#13. What happens if linear regression assumptions are violated\n",
    "''' If linear regression assumptions are violated, the model's estimates and predictions may be biased or unreliable. \n",
    "We need to check and address any violations before interpreting the results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7a0837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' We can address multicollinearity by:\\n\\nRemoving one of the highly correlated independent variables\\nUsing dimensionality reduction techniques (e.g., PCA)\\nUsing regularization techniques (e.g., Ridge or Lasso regression)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#14. How can we address multicollinearity in regression\n",
    "''' We can address multicollinearity by:\n",
    "\n",
    "Removing one of the highly correlated independent variables\n",
    "Using dimensionality reduction techniques (e.g., PCA)\n",
    "Using regularization techniques (e.g., Ridge or Lasso regression)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37623a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFeature selection can improve model performance by:\\n\\nReducing multicollinearity\\nIncreasing the model's interpretability\\nReducing overfitting\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#15. How can feature selection improve model performance in regression analysis\n",
    "\n",
    "'''\n",
    "Feature selection can improve model performance by:\n",
    "\n",
    "Reducing multicollinearity\n",
    "Increasing the model's interpretability\n",
    "Reducing overfitting'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7661c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Adjusted R-squared is calculated as: 1 - ((n-1)/(n-k-1)) * (1-R-squared), where n is the number of observations and k is the number of independent variables'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#16. How is Adjusted R-squared calculated:\n",
    "\n",
    "'''  Adjusted R-squared is calculated as: 1 - ((n-1)/(n-k-1)) * (1-R-squared), where n is the number of observations and k is the number of independent variables'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f0d8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' MSE is sensitive to outliers because it squares the differences between predicted and actual values, which can amplify the effect of outliers.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#17. Why is MSE sensitive to outliers:\n",
    "\n",
    "''' MSE is sensitive to outliers because it squares the differences between predicted and actual values, which can amplify the effect of outliers.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af17a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Homoscedasticity is the assumption that the variance of the residuals is constant across all levels of the independent variable(s). \\nThis is important because it allows us to make reliable inferences about the relationship between the independent and dependent variables.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#18. What is the role of homoscedasticity in linear regression:\n",
    "\n",
    "\n",
    "''' Homoscedasticity is the assumption that the variance of the residuals is constant across all levels of the independent variable(s). \n",
    "This is important because it allows us to make reliable inferences about the relationship between the independent and dependent variables.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f3541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RMSE measures the square root of the average squared difference between predicted and actual values, providing a measure of the model's accuracy.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#19. What is Root Mean Squared Error (RMSE)\n",
    "'''RMSE measures the square root of the average squared difference between predicted and actual values, providing a measure of the model's accuracy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be7db262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pickling is considered risky because it can lead to security vulnerabilities if not done properly.\\n Pickled objects can contain arbitrary code, which can be executed when the object is loaded.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20. Why is pickling considered risky\n",
    "\n",
    "\n",
    "'''Pickling is considered risky because it can lead to security vulnerabilities if not done properly.\n",
    " Pickled objects can contain arbitrary code, which can be executed when the object is loaded.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dec1fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Alternatives to pickling include:\\n\\nSaving ML models as text files (e.g., JSON or YAML)\\nUsing a model repository (e.g., ModelHub or MLflow)\\nUsing a cloud-based ML platform (e.g., AWS SageMaker or Google Cloud AI Platform)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#21. What alternatives exist to pickling for saving ML models\n",
    "\n",
    "''' Alternatives to pickling include:\n",
    "\n",
    "Saving ML models as text files (e.g., JSON or YAML)\n",
    "Using a model repository (e.g., ModelHub or MLflow)\n",
    "Using a cloud-based ML platform (e.g., AWS SageMaker or Google Cloud AI Platform)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91ed0317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heteroscedasticity is the situation where the variance of the residuals is not constant across all levels of the independent variable(s).\\nThis is a problem because it can lead to biased or unreliable estimates and predictions.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#22. What is heteroscedasticity, and why is it a problem: \n",
    "\n",
    "\n",
    "'''Heteroscedasticity is the situation where the variance of the residuals is not constant across all levels of the independent variable(s).\n",
    "This is a problem because it can lead to biased or unreliable estimates and predictions.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9069e5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Interaction terms can enhance a regression model's predictive power by:\\n\\nCapturing non-linear relationships between the independent variables and the dependent variable\\nAllowing for more complex and nuanced relationships between the variables\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#23. How can interaction terms enhance a regression model's predictive power\n",
    "'''Interaction terms can enhance a regression model's predictive power by:\n",
    "\n",
    "Capturing non-linear relationships between the independent variables and the dependent variable\n",
    "Allowing for more complex and nuanced relationships between the variables'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1cfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
